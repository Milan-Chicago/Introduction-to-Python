{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bacterial-criminal",
   "metadata": {},
   "source": [
    "# Machine Learning Basics with Scikit-learn: Day 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "large-lounge",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-diamond",
   "metadata": {},
   "source": [
    "### Objectives\n",
    "The workshop will focus on the basics of *scikit-learn*, which is one of the most popular machine learning libraries in Python. After the workshop you will:\n",
    "\n",
    "* Understand how to use scikit-learn functions and documentation.\n",
    "* Learn the usual procedure to transform your data for a machine learning model.\n",
    "* Create basic machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "international-customs",
   "metadata": {},
   "source": [
    "### Why scikit-learn?\n",
    "Scikit-learn is an open source machine learning library that supports supervised and unsupervised learning. It also provides various tools for model fitting, data preprocessing, model selection and evaluation, and many other utilities. It has a standardized and simple interface for preprocessing data and model training, optimization and evaluation. \n",
    "\n",
    "### Why are we teaching machine learning with scikit-learn?\n",
    "Using machine learning is becoming a mandatory new skill for many professional workers. As work and organizations demand more use of data analyses, machine learning allows us to analyze large datasets to extract meaningful information. Although machine learning techniques have been there for decades, modern programming languages are making these techniques available for thousands of users. Rather than developing each method from scratch, *scikit-learn* offers a simple way to implement them with our datasets. This package provides a easy toolkit to master and leverage machine learning skills. We hope that learning sckikit-learn helps you comprehend the overall process of data transformation: from curating and importing datasets, to curating models for data knowledge purposes. \n",
    "\n",
    "### Structure of the Workshop\n",
    "The workshop is divided into 5 days:\n",
    "\n",
    "1. Introduction to scikit-learn\n",
    "2. Supervisded learning: Classification models\n",
    "3. Unsupervised learning: Clustering models\n",
    "4. Data cleaning / transformation\n",
    "5. Model selection\n",
    "\n",
    "Today, we will start with an overview of *scikit-learn.* We will load datasets, create training and testing datasets, create a model, and evaluate it. We will delve into the functions, models, and details these days. However, keep in mind that this workshop's content will stay at an introductory level. If you are interested in learning more, here are some good resources that you can explore:\n",
    "* [Official documentation](https://scikit-learn.org/stable/index.html)\n",
    "* [Google Cloud AI Adventures](https://www.youtube.com/hashtag/aiadventures)\n",
    "* [Google's Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course)\n",
    "* [Stanford University: CS229: Machine Learning](http://cs229.stanford.edu/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separate-spain",
   "metadata": {},
   "source": [
    "## Installing scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlikely-capitol",
   "metadata": {},
   "source": [
    "Conda and Google Colab already have scikit-learn. You should skip the instalation if you're using one of those. If you are running this notebook on your own environment, please run the following command. And for more instructions, please check out the [documentation](https://scikit-learn.org/stable/install.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "regular-cleaners",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you do not have scikit-learn installed, uncomment the following lines and run the following command. \n",
    "# import sys\n",
    "# !{sys.executable} -m pip install scikit-learn==0.24.2\n",
    "# !{sys.executable} -m pip install scikit-learn --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "small-packaging",
   "metadata": {},
   "source": [
    "Let's check that the package is in your environment. Run the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "through-bridal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "System:\n",
      "    python: 3.9.1 (default, Feb  1 2021, 20:41:56)  [Clang 12.0.0 (clang-1200.0.32.29)]\n",
      "executable: /usr/local/opt/python@3.9/bin/python3.9\n",
      "   machine: macOS-10.15.7-x86_64-i386-64bit\n",
      "\n",
      "Python dependencies:\n",
      "          pip: 21.1.3\n",
      "   setuptools: 52.0.0\n",
      "      sklearn: 0.24.1\n",
      "        numpy: 1.20.0\n",
      "        scipy: 1.6.1\n",
      "       Cython: None\n",
      "       pandas: 1.2.1\n",
      "   matplotlib: 3.3.4\n",
      "       joblib: 1.0.1\n",
      "threadpoolctl: 2.1.0\n",
      "\n",
      "Built with OpenMP: True\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.show_versions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considerable-therapy",
   "metadata": {},
   "source": [
    "## The big picture\n",
    "One of the biggest advantages of machine learning is to determine how to differentiate observations using a computational model, rather than using human coding and manual rules. When we have thousands of obsevations, machine learning models helps us automatize, scale, and guarantee the reproducibility these coding processes. The basic steps are:\n",
    "1. Gathering data\n",
    "2. Preparing that data\n",
    "3. Choosing a model\n",
    "4. Training\n",
    "5. Evaluation\n",
    "6. Prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acute-paper",
   "metadata": {},
   "source": [
    "## 1. Gathering data\n",
    "\n",
    "The first step is collecting data and understanding the dataset. This step is very important because the quality and quantity of data that you gather will directly determine how good your predictive model can be. Also, many machine learning models are *biased* because of the data. A well-known example is [this face-generator model](https://www.theverge.com/21298762/face-depixelizer-ai-machine-learning-tool-pulse-stylegan-obama-bias) that did not recognize President Obama as an African-American person. \n",
    "\n",
    "You need to collect data and take into account these recommendations:\n",
    "* **Number of observations (*N*):** In statistics, there is a theorem called \"Law of Big Numbers.\" According to this law, the average of the results obtained from a large number of trials should be close to the expected value. In other words, by having a large number of observations, your model will tend to become closer to the expected value. Creating machine learning models is based on *big datasets.* Imagine companies that collect big data from their clients and are able to get a clear picture of them. Prediciton becomes more accurate as long as you have more observations and knowledge of your population. \n",
    "* **Missing data**: Many observations may lack some values. You can have multiple reasons for this: data-collection issues, restricted data, information not available, etc. It is important to make strategies whenever you have missing data. \n",
    "* **Representativity**: One main problem in most datasets is checking how representative is the dataset with respect to the population. How can we be sure that the dataset is not baised? Computers do not understand the dataset context, or how the dataset was collected. This problem lies on the people who collected the data. Moreover, people who got the data must be aware about any potential flaws or inequealities in the dataset. Descriptive analysis should guide any checkings and validation processes. \n",
    "\n",
    "In this workshop, we will use the datasets provided by *scikit-learn*. You can check other [toy datasets](https://scikit-learn.org/stable/datasets/toy_dataset.html) if you are interested in exploring more. These datasets are well-known, public, and used frequently for learning and testing purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configured-telling",
   "metadata": {},
   "source": [
    "### Loading datasets\n",
    "\n",
    "Before we get started, some terms that we must get familiar with:\n",
    "* **Samples**: A sample is an observation available in the dataset. Also, they are known as observations, records, etc. They are usually the rows of a dataset table.\n",
    "* **Features**: A feature is an individual measurable property. We also know them as variables and attributes. Usually, these are the columns of a dataset table.\n",
    "\n",
    "We will start importing the scikit-learn's datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "foreign-mirror",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acceptable-palace",
   "metadata": {},
   "source": [
    "All datasets are now in the environment. We can call specifically one of those and assign them as a variable. We will use the *Boston house prices dataset.* Each record in the database describes a Boston suburb or town. The data was drawn from the Boston Standard Metropolitan Statistical Area (SMSA) in 1970. The attributes are deﬁned as follows (taken from the UCI Machine Learning Repository1): CRIM: per capita crime rate by town\n",
    "\n",
    "* CRIM: crime per capita crime rate by town\n",
    "* ZN: proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "* INDUS: proportion of non-retail business acres per town\n",
    "* CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "* NOX: nitric oxides concentration (parts per 10 million)\n",
    "* RM: average number of rooms per dwelling\n",
    "* AGE: proportion of owner-occupied units built prior to 1940\n",
    "* DIS: weighted distances to five Boston employment centres\n",
    "* RAD: index of accessibility to radial highways\n",
    "* TAX: full-value property-tax rate per \\$10,000\n",
    "* PTRATIO: pupil-teacher ratio by town\n",
    "* B: 1000(Bk - 0.63)^2 where Bk is the proportion of black people by town\n",
    "* LSTAT: proportion of lower status of the population\n",
    "* MEDV: Median value of owner-occupied homes in $1000’s\n",
    "\n",
    "This model has been frequently used for regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "otherwise-quantity",
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = datasets.load_boston()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handy-instrument",
   "metadata": {},
   "source": [
    "## 2. Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "humanitarian-going",
   "metadata": {},
   "source": [
    "In a machine learning model, we will have *features* and a *target*. The target is the variable to predict/estimate by the model. The value of the target depends on the features' values. Usually, targets are the outcomes of a process (e.g., giving a loan, earnings). In statistics, the target is also known as the *dependent variable*, and the features are the *independent variables*.\n",
    "\n",
    "![In this example, demographic informations are used to predict users' behavior when they are navigating on a website](https://d2m6ke2px6quvq.cloudfront.net/uploads/2020/09/11/0e1df989-5fc9-474b-ba49-5eaebfc2d795.png)\n",
    "\n",
    "In this example, users' demographic and activity information are used to predict users' behavior when they are navigating on a website."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "healthy-greenhouse",
   "metadata": {},
   "source": [
    "### Getting the features and target data from scikit-learn\n",
    "*scikit-learn* datasets are formated as dictionary-like objects. They hold all the data and some metadata about the data. These data are stored in the `.data` member, which is a `n_samples`, `n_features` array. \n",
    "\n",
    "The member `.feature_names` will contain the names recorded for each feature.\n",
    "\n",
    "In the case of supervised problem, one or more response variables are stored in the `.target` member. These are variables to *predict*.\n",
    "\n",
    "Let's check the Boston dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "exact-crowd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.3200e-03 1.8000e+01 2.3100e+00 ... 1.5300e+01 3.9690e+02 4.9800e+00]\n",
      " [2.7310e-02 0.0000e+00 7.0700e+00 ... 1.7800e+01 3.9690e+02 9.1400e+00]\n",
      " [2.7290e-02 0.0000e+00 7.0700e+00 ... 1.7800e+01 3.9283e+02 4.0300e+00]\n",
      " ...\n",
      " [6.0760e-02 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9690e+02 5.6400e+00]\n",
      " [1.0959e-01 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9345e+02 6.4800e+00]\n",
      " [4.7410e-02 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9690e+02 7.8800e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(boston.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "homeless-collapse",
   "metadata": {},
   "source": [
    "In this dataset, the **Median Value** (the last attribute) is usually the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "objective-judgment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24.  21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 15.  18.9 21.7 20.4\n",
      " 18.2 19.9 23.1 17.5 20.2 18.2 13.6 19.6 15.2 14.5 15.6 13.9 16.6 14.8\n",
      " 18.4 21.  12.7 14.5 13.2 13.1 13.5 18.9 20.  21.  24.7 30.8 34.9 26.6\n",
      " 25.3 24.7 21.2 19.3 20.  16.6 14.4 19.4 19.7 20.5 25.  23.4 18.9 35.4\n",
      " 24.7 31.6 23.3 19.6 18.7 16.  22.2 25.  33.  23.5 19.4 22.  17.4 20.9\n",
      " 24.2 21.7 22.8 23.4 24.1 21.4 20.  20.8 21.2 20.3 28.  23.9 24.8 22.9\n",
      " 23.9 26.6 22.5 22.2 23.6 28.7 22.6 22.  22.9 25.  20.6 28.4 21.4 38.7\n",
      " 43.8 33.2 27.5 26.5 18.6 19.3 20.1 19.5 19.5 20.4 19.8 19.4 21.7 22.8\n",
      " 18.8 18.7 18.5 18.3 21.2 19.2 20.4 19.3 22.  20.3 20.5 17.3 18.8 21.4\n",
      " 15.7 16.2 18.  14.3 19.2 19.6 23.  18.4 15.6 18.1 17.4 17.1 13.3 17.8\n",
      " 14.  14.4 13.4 15.6 11.8 13.8 15.6 14.6 17.8 15.4 21.5 19.6 15.3 19.4\n",
      " 17.  15.6 13.1 41.3 24.3 23.3 27.  50.  50.  50.  22.7 25.  50.  23.8\n",
      " 23.8 22.3 17.4 19.1 23.1 23.6 22.6 29.4 23.2 24.6 29.9 37.2 39.8 36.2\n",
      " 37.9 32.5 26.4 29.6 50.  32.  29.8 34.9 37.  30.5 36.4 31.1 29.1 50.\n",
      " 33.3 30.3 34.6 34.9 32.9 24.1 42.3 48.5 50.  22.6 24.4 22.5 24.4 20.\n",
      " 21.7 19.3 22.4 28.1 23.7 25.  23.3 28.7 21.5 23.  26.7 21.7 27.5 30.1\n",
      " 44.8 50.  37.6 31.6 46.7 31.5 24.3 31.7 41.7 48.3 29.  24.  25.1 31.5\n",
      " 23.7 23.3 22.  20.1 22.2 23.7 17.6 18.5 24.3 20.5 24.5 26.2 24.4 24.8\n",
      " 29.6 42.8 21.9 20.9 44.  50.  36.  30.1 33.8 43.1 48.8 31.  36.5 22.8\n",
      " 30.7 50.  43.5 20.7 21.1 25.2 24.4 35.2 32.4 32.  33.2 33.1 29.1 35.1\n",
      " 45.4 35.4 46.  50.  32.2 22.  20.1 23.2 22.3 24.8 28.5 37.3 27.9 23.9\n",
      " 21.7 28.6 27.1 20.3 22.5 29.  24.8 22.  26.4 33.1 36.1 28.4 33.4 28.2\n",
      " 22.8 20.3 16.1 22.1 19.4 21.6 23.8 16.2 17.8 19.8 23.1 21.  23.8 23.1\n",
      " 20.4 18.5 25.  24.6 23.  22.2 19.3 22.6 19.8 17.1 19.4 22.2 20.7 21.1\n",
      " 19.5 18.5 20.6 19.  18.7 32.7 16.5 23.9 31.2 17.5 17.2 23.1 24.5 26.6\n",
      " 22.9 24.1 18.6 30.1 18.2 20.6 17.8 21.7 22.7 22.6 25.  19.9 20.8 16.8\n",
      " 21.9 27.5 21.9 23.1 50.  50.  50.  50.  50.  13.8 13.8 15.  13.9 13.3\n",
      " 13.1 10.2 10.4 10.9 11.3 12.3  8.8  7.2 10.5  7.4 10.2 11.5 15.1 23.2\n",
      "  9.7 13.8 12.7 13.1 12.5  8.5  5.   6.3  5.6  7.2 12.1  8.3  8.5  5.\n",
      " 11.9 27.9 17.2 27.5 15.  17.2 17.9 16.3  7.   7.2  7.5 10.4  8.8  8.4\n",
      " 16.7 14.2 20.8 13.4 11.7  8.3 10.2 10.9 11.   9.5 14.5 14.1 16.1 14.3\n",
      " 11.7 13.4  9.6  8.7  8.4 12.8 10.5 17.1 18.4 15.4 10.8 11.8 14.9 12.6\n",
      " 14.1 13.  13.4 15.2 16.1 17.8 14.9 14.1 12.7 13.5 14.9 20.  16.4 17.7\n",
      " 19.5 20.2 21.4 19.9 19.  19.1 19.1 20.1 19.9 19.6 23.2 29.8 13.8 13.3\n",
      " 16.7 12.  14.6 21.4 23.  23.7 25.  21.8 20.6 21.2 19.1 20.6 15.2  7.\n",
      "  8.1 13.6 20.1 21.8 24.5 23.1 19.7 18.3 21.2 17.5 16.8 22.4 20.6 23.9\n",
      " 22.  11.9]\n"
     ]
    }
   ],
   "source": [
    "print(boston.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "careful-moderator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
      " 'B' 'LSTAT']\n"
     ]
    }
   ],
   "source": [
    "print(boston.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "christian-princess",
   "metadata": {},
   "source": [
    "Before applying any model, it is a good idea to convert this data structure to a pandas dataframe. We can keeep the data in one single object and keep the features' names. We can do by calling the dataframe on `boston.data` and adding the target variable to the dataframe from `boston.target`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cardiac-cleaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "boston_df = pd.DataFrame(boston.data, columns = boston.feature_names)\n",
    "boston_df['PRICE'] = boston.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interpreted-slave",
   "metadata": {},
   "source": [
    "We will print the first 5 columns of this pandas dataframe. We will see each column with their respective feature name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "phantom-final",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
      "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
      "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
      "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
      "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
      "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
      "\n",
      "   PTRATIO       B  LSTAT  PRICE  \n",
      "0     15.3  396.90   4.98   24.0  \n",
      "1     17.8  396.90   9.14   21.6  \n",
      "2     17.8  392.83   4.03   34.7  \n",
      "3     18.7  394.63   2.94   33.4  \n",
      "4     18.7  396.90   5.33   36.2  \n"
     ]
    }
   ],
   "source": [
    "print(boston_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hindu-assistant",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "Before we create a model, let's get familiar with the target column, \"PRICE,\" which is the value of prices of the houses. Run the following command and check the mean, minimum value, maximum value, and the 50\\%. How is the data distributed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "shared-flesh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    506.000000\n",
      "mean      22.532806\n",
      "std        9.197104\n",
      "min        5.000000\n",
      "25%       17.025000\n",
      "50%       21.200000\n",
      "75%       25.000000\n",
      "max       50.000000\n",
      "Name: PRICE, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(boston_df['PRICE'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stock-japanese",
   "metadata": {},
   "source": [
    "## 3. Training and Testing sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-breakfast",
   "metadata": {},
   "source": [
    "To see if the model is capable to predict \"future\" values, we split the original dataset in two sets: one portion will be used for **training**, and the second portion will be for **testing** purposes. The testing dataset will *test* the model against data that has never been used for *training*. Having a testing dataset allows us to see how the model might perform against data that it has not yet seen. This is meant to be representative of how the model might perform in the real world.\n",
    "\n",
    "A rule of thumb is use for a training-evaluation split somewhere on the order of 80/20, 90/10. Much of this depends on the size of the original source dataset. We will start training the model with 80% of the sample and test the model with the remaining 20%. We do this to assess the model's performance on unseen data.\n",
    "\n",
    "We must separate the features that will act as independent variables (`X`) from the target (`Y`). The independent variables include all attributes but `'PRICE'`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "three-twins",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = boston_df.drop('PRICE', axis = 1)\n",
    "y = boston_df['PRICE']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corresponding-click",
   "metadata": {},
   "source": [
    "To split the data, we use `train_test_split` function provided by *scikit-learn* library. We finally print the shapes of our training and test set to verify if the splitting has occurred properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "single-allen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the train_test_split function\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "comprehensive-panel",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "japanese-tourist",
   "metadata": {},
   "source": [
    "Now, we check the dimensions of both sets are correct. Since we have 506 observations, the training set has ~404 observations (80%) and the testing set has about 102 observations. Since we have 13 features in total, each `X` dataframe has 13 columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "affiliated-slide",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404, 13)\n",
      "(102, 13)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surface-indicator",
   "metadata": {},
   "source": [
    "Finally, the target is a single column with 404 observations for the training set, and 102 observations for the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cooperative-supplier",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404,)\n",
      "(102,)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improved-disability",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "Change the proportion for the training set. Instead of 80\\%, set it up for **90%** (i.e., 90% for training and 10% for testing). How many observations would you have for the traning and testing datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "floral-convention",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the code here\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X, y, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "athletic-bronze",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(455, 13)\n",
      "(51, 13)\n"
     ]
    }
   ],
   "source": [
    "# Print X datasets' shape\n",
    "print(X_train_2.shape)\n",
    "print(X_test_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dressed-episode",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(455,)\n",
      "(51,)\n"
     ]
    }
   ],
   "source": [
    "# Print y datasets' shape\n",
    "print(y_train_2.shape)\n",
    "print(y_test_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-blair",
   "metadata": {},
   "source": [
    "## 4. Choosing a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eligible-shark",
   "metadata": {},
   "source": [
    "The next step in our workflow is choosing a model. There are many models that researchers and data scientists have created over the years. Some are very well suited for image data, others for sequences (like text, or music), some for numerical data, others for text-based data. \n",
    "\n",
    "Since we have 13 features, one target, and numerical data, we can use a **linear regression model.** Regression models are very useful for numeric datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "helpful-hello",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Linear Regression model\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "interstate-tolerance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funky-award",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "Run again the lineal regression model using your training set with 90% of the observations (`X_train_2`,`y_train_2`). Call your model `reg2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dressed-virus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run regression model\n",
    "reg2 = LinearRegression()\n",
    "reg2.fit(X_train_2, y_train_2) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fixed-lending",
   "metadata": {},
   "source": [
    "## 5. Evaluating your model\n",
    "\n",
    "Once training is complete, it's time to see if the model provides accurate results. A good practice is to compare the original values (`y_train`) with the predicted values given by the model. We want to check how *good* is the model predicitng the values were used for training. \n",
    "\n",
    "We use the function `predict` to get the values according to the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "structural-channel",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_predict = reg.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-germany",
   "metadata": {},
   "source": [
    "We compare the predicted training values with the real training values by checking their difference. If the difference is small, the model is predicting values close to the real ones. We will create a table now to compare them visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "religious-first",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_real</th>\n",
       "      <th>y_predicted</th>\n",
       "      <th>difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>23.1</td>\n",
       "      <td>20.842998</td>\n",
       "      <td>2.257002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>50.0</td>\n",
       "      <td>22.971757</td>\n",
       "      <td>27.028243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>18.5</td>\n",
       "      <td>25.020635</td>\n",
       "      <td>-6.520635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>21.7</td>\n",
       "      <td>21.559434</td>\n",
       "      <td>0.140566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>28.1</td>\n",
       "      <td>25.634705</td>\n",
       "      <td>2.465295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>19.5</td>\n",
       "      <td>17.145487</td>\n",
       "      <td>2.354513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>20.1</td>\n",
       "      <td>20.072545</td>\n",
       "      <td>0.027455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>22.6</td>\n",
       "      <td>26.493185</td>\n",
       "      <td>-3.893185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>12.5</td>\n",
       "      <td>19.112699</td>\n",
       "      <td>-6.612699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>46.7</td>\n",
       "      <td>36.017114</td>\n",
       "      <td>10.682886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     y_real  y_predicted  difference\n",
       "16     23.1    20.842998    2.257002\n",
       "368    50.0    22.971757   27.028243\n",
       "114    18.5    25.020635   -6.520635\n",
       "357    21.7    21.559434    0.140566\n",
       "213    28.1    25.634705    2.465295\n",
       "106    19.5    17.145487    2.354513\n",
       "286    20.1    20.072545    0.027455\n",
       "174    22.6    26.493185   -3.893185\n",
       "396    12.5    19.112699   -6.612699\n",
       "228    46.7    36.017114   10.682886"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_df = pd.DataFrame({'y_real': y_train, 'y_predicted': y_train_predict})\n",
    "y_train_df['difference'] = y_train_df.y_real - y_train_df.y_predicted \n",
    "y_train_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "martial-interaction",
   "metadata": {},
   "source": [
    "As expected, some predicted values are close to the real values (i.e., their differences are close to zero), and some others are far from expected (i.e., big values in their differences). \n",
    "\n",
    "We need **metrics** to evaluate empirically how good is this model. For linear regression models, one metric is the *coefficient of determination* (R^2) of the prediction. This score is related to the differences between the predicted values and the original values. The best possible score is 1.0. For more details, click [here](https://en.wikipedia.org/wiki/Coefficient_of_determination). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "burning-acrobat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score is 0.74\n"
     ]
    }
   ],
   "source": [
    "r2 = round(reg.score(X_train, y_train),2)\n",
    "print('R2 score is {}'.format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brown-baltimore",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "Calcualte the predicted value using the model `reg2` with the 90% training dataset. Use `y_train_predict_2` for the new predicted values. Then, copy the pandas dataframe and replace the files with your 90% datasets (`y_train_2` and `y_train_predict_2`). The new dataframe should be called `y_train_df_2`. Print `y_train_df_2` dataframe's head. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "concrete-spine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_real</th>\n",
       "      <th>y_predicted</th>\n",
       "      <th>difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>22.1</td>\n",
       "      <td>26.612465</td>\n",
       "      <td>-4.512465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>18.7</td>\n",
       "      <td>18.045837</td>\n",
       "      <td>0.654163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>18.0</td>\n",
       "      <td>18.672194</td>\n",
       "      <td>-0.672194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>22.5</td>\n",
       "      <td>29.160456</td>\n",
       "      <td>-6.660456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>23.2</td>\n",
       "      <td>16.952115</td>\n",
       "      <td>6.247885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>20.5</td>\n",
       "      <td>24.280924</td>\n",
       "      <td>-3.780924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>21.7</td>\n",
       "      <td>20.254179</td>\n",
       "      <td>1.445821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34.7</td>\n",
       "      <td>30.780529</td>\n",
       "      <td>3.919471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>23.2</td>\n",
       "      <td>25.492126</td>\n",
       "      <td>-2.292126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>22.9</td>\n",
       "      <td>20.521041</td>\n",
       "      <td>2.378959</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     y_real  y_predicted  difference\n",
       "311    22.1    26.612465   -4.512465\n",
       "60     18.7    18.045837    0.654163\n",
       "128    18.0    18.672194   -0.672194\n",
       "298    22.5    29.160456   -6.660456\n",
       "391    23.2    16.952115    6.247885\n",
       "51     20.5    24.280924   -3.780924\n",
       "110    21.7    20.254179    1.445821\n",
       "2      34.7    30.780529    3.919471\n",
       "176    23.2    25.492126   -2.292126\n",
       "350    22.9    20.521041    2.378959"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the predicted prices based on the 90% training set\n",
    "y_train_predict_2 = reg2.predict(X_train_2)\n",
    "\n",
    "# Create the dataframe\n",
    "y_train_df_2 = pd.DataFrame({'y_real': y_train_2, 'y_predicted': y_train_predict_2})\n",
    "y_train_df_2['difference'] = y_train_df_2.y_real - y_train_df_2.y_predicted \n",
    "y_train_df_2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decimal-injury",
   "metadata": {},
   "source": [
    "Compute the R^2 of your model and compare it with the previous model' R^2. Is it better or worse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "finnish-kingdom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score is 0.75\n"
     ]
    }
   ],
   "source": [
    "r2 = round(reg.score(X_train_2, y_train_2),2)\n",
    "print('R2 score is {}'.format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-exhibit",
   "metadata": {},
   "source": [
    "## 6. Prediction\n",
    "In this final step, we use the model that we trained to *predict* new values (or values that were not tested before). This is the final test to check how good the model is. If the phenomena or casual effects are reflected correctly in our model, then the model will be capable to predict new observations. If the model's performance gets worse, we must reconsider the training dataset, features, and machine learning model used. \n",
    "\n",
    "Like the prior step, we first start predicting the target values (`y_test_predict`) from the testing dataset (`X_test`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "limited-pilot",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predict = reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diverse-flashing",
   "metadata": {},
   "source": [
    "We compare the predicted testing values with the real testing values by checking their difference. If the difference is small, the model is predicting values close to the real ones. We will create a table now to compare the numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "catholic-reception",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_original</th>\n",
       "      <th>y_predicted</th>\n",
       "      <th>difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>26.6</td>\n",
       "      <td>22.063683</td>\n",
       "      <td>4.536317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>20.6</td>\n",
       "      <td>22.441019</td>\n",
       "      <td>-1.841019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>23.9</td>\n",
       "      <td>26.729543</td>\n",
       "      <td>-2.829543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>50.0</td>\n",
       "      <td>24.694124</td>\n",
       "      <td>25.305876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>23.2</td>\n",
       "      <td>22.243383</td>\n",
       "      <td>0.956617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>18.9</td>\n",
       "      <td>18.662283</td>\n",
       "      <td>0.237717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>20.6</td>\n",
       "      <td>21.217312</td>\n",
       "      <td>-0.617312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>19.4</td>\n",
       "      <td>17.381433</td>\n",
       "      <td>2.018567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>19.0</td>\n",
       "      <td>21.455078</td>\n",
       "      <td>-2.455078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>22.5</td>\n",
       "      <td>22.433027</td>\n",
       "      <td>0.066973</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     y_original  y_predicted  difference\n",
       "349        26.6    22.063683    4.536317\n",
       "502        20.6    22.441019   -1.841019\n",
       "81         23.9    26.729543   -2.829543\n",
       "372        50.0    24.694124   25.305876\n",
       "472        23.2    22.243383    0.956617\n",
       "9          18.9    18.662283    0.237717\n",
       "487        20.6    21.217312   -0.617312\n",
       "153        19.4    17.381433    2.018567\n",
       "339        19.0    21.455078   -2.455078\n",
       "86         22.5    22.433027    0.066973"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_df = pd.DataFrame({'y_original': y_test, 'y_predicted': y_test_predict})\n",
    "y_test_df['difference'] = y_test_df.y_original - y_test_df.y_predicted \n",
    "y_test_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spread-plasma",
   "metadata": {},
   "source": [
    "Finally, we compute the R^2 of this model and check its performance. We can expect to see a lower performance compared to the training validation. We will discuss in another session how we can make these models have better scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "interim-shock",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.73\n"
     ]
    }
   ],
   "source": [
    "r2 = round(reg.score(X_test, y_test),2)\n",
    "print(\"R^2: {}\".format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creative-ordinance",
   "metadata": {},
   "source": [
    "### Exercise 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "headed-blond",
   "metadata": {},
   "source": [
    "Calcualte the predicted value using the model `reg2` with the 90% testing dataset. Use `y_test_predict_2` for the new predicted values. Then, copy the pandas dataframe and replace the variables with your 90% testing dataset (`y_test_2` and `y_test_predict_2`). The new dataframe should be called `y_test_df_2`. Print `y_test_df_2` dataframe's head. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "included-uncertainty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the predicted value\n",
    "y_test_predict_2 = reg2.predict(X_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "administrative-peeing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_original</th>\n",
       "      <th>y_predicted</th>\n",
       "      <th>difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>17.4</td>\n",
       "      <td>22.749264</td>\n",
       "      <td>-5.349264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>18.9</td>\n",
       "      <td>21.955882</td>\n",
       "      <td>-3.055882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>20.1</td>\n",
       "      <td>18.187989</td>\n",
       "      <td>1.912011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>13.1</td>\n",
       "      <td>13.606285</td>\n",
       "      <td>-0.506285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>36.1</td>\n",
       "      <td>33.140070</td>\n",
       "      <td>2.959930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>30.7</td>\n",
       "      <td>31.379309</td>\n",
       "      <td>-0.679309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>21.0</td>\n",
       "      <td>20.943495</td>\n",
       "      <td>0.056505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>18.6</td>\n",
       "      <td>20.061146</td>\n",
       "      <td>-1.461146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>25.0</td>\n",
       "      <td>25.650152</td>\n",
       "      <td>-0.650152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>22.8</td>\n",
       "      <td>28.732440</td>\n",
       "      <td>-5.932440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     y_original  y_predicted  difference\n",
       "170        17.4    22.749264   -5.349264\n",
       "11         18.9    21.955882   -3.055882\n",
       "469        20.1    18.187989    1.912011\n",
       "156        13.1    13.606285   -0.506285\n",
       "304        36.1    33.140070    2.959930\n",
       "266        30.7    31.379309   -0.679309\n",
       "29         21.0    20.943495    0.056505\n",
       "102        18.6    20.061146   -1.461146\n",
       "165        25.0    25.650152   -0.650152\n",
       "308        22.8    28.732440   -5.932440"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the dataframe\n",
    "y_test_df_2 = pd.DataFrame({'y_original': y_test_2, 'y_predicted': y_test_predict_2})\n",
    "y_test_df_2['difference'] = y_test_df_2.y_original - y_test_df_2.y_predicted \n",
    "y_test_df_2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-lighter",
   "metadata": {},
   "source": [
    "Compute the R2 of your model and compare it with the previous model' R2. Is it better or worse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "floppy-excess",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score is 0.68\n"
     ]
    }
   ],
   "source": [
    "r2 = round(reg2.score(X_test_2, y_test_2),2)\n",
    "print('R2 score is {}'.format(r2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
